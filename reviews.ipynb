{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import operator\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim.models import Phrases,Word2Vec\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten,SpatialDropout1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D,MaxPooling1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "\n",
    "#%conda install -c anaconda gensim\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all():\n",
    "    df = pd.read_csv('./imdb_master.csv',encoding=\"latin-1\")\n",
    "    df.drop(['Unnamed: 0','file','type'],axis=1,inplace=True)\n",
    "    df = df[df.label != 'unsup']\n",
    "    df.label = df.label.map({'pos':1,'neg':0})\n",
    "\n",
    "\n",
    "    df1 = pd.read_csv('./labeledTrainData.tsv', delimiter='\\t')\n",
    "    df1.drop(['id'],axis=1,inplace=True)\n",
    "    df1.columns = ['label','review']\n",
    "\n",
    "\n",
    "    X_train = pd.concat([df, df1]).reset_index(drop=True)\n",
    "    y_train = X_train.label\n",
    "    X_train.drop(['label'],axis=1,inplace=True)\n",
    "\n",
    "    X_test = pd.read_csv('./testData.tsv', delimiter='\\t')\n",
    "    id_test = X_test.id\n",
    "    X_test.drop(['id'],axis=1,inplace=True)\n",
    "    \n",
    "    return X_train,X_test,y_train,id_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(text):\n",
    "    soup = BeautifulSoup(text,'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def rm_special(text):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "def lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    text = ' '.join([lemmatizer.lemmatize(word).lower() for word in text.split() if word not in stop])\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    X_train['review'] = X_train['review'].apply(get_text).apply(rm_special).apply(lemma)\n",
    "    X_test['review'] = X_test['review'].apply(get_text).apply(rm_special).apply(lemma)\n",
    "\n",
    "    return X_train,X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(X_train,X_test):\n",
    "    NGRAM_RANGE = (1, 3)\n",
    "    TOP_K = 20000\n",
    "    TOKEN_MODE = 'word'\n",
    "    MIN_DOC_FREQ = 2\n",
    "\n",
    "    kwargs = {\n",
    "            'ngram_range' : NGRAM_RANGE,\n",
    "            'dtype' : 'int32',\n",
    "            'strip_accents' : 'unicode',\n",
    "            'decode_error' : 'replace',\n",
    "            'analyzer' : TOKEN_MODE,\n",
    "            'min_df' : MIN_DOC_FREQ,\n",
    "        }\n",
    "    tv=TfidfVectorizer(**kwargs)\n",
    "    train_data = tv.fit_transform(X_train['review'])\n",
    "    test_data = tv.transform(X_test['review'])\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, train_data.shape[1]))\n",
    "    train_data  = selector.fit_transform(train_data, y_train)\n",
    "    test_data= selector.transform(test_data)\n",
    "    return train_data,test_data\n",
    "#X_train,X_test = vectorize(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "nb = MultinomialNB()\n",
    "#cross_val_score(nb,X_train,y_train,cv=5,scoring='accuracy').mean()\n",
    "nb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiNB - 0.904 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "results = pd.DataFrame({\"id\": id_test, \"sentiment\": y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense+ Embedding layer - 0.981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,id_test = load_all()\n",
    "X_train,X_test= prepare()\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['review'])\n",
    "max_len = max([len(s.split()) for s in X_train['review']])\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "#model.summary()\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D + Embedding layer - 0.961"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,id_test = load_all()\n",
    "X_train,X_test= prepare()\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['review'])\n",
    "max_len = max([len(s.split()) for s in X_train['review']])\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 15000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 583s 10ms/step - loss: 0.3497 - accuracy: 0.8239 - val_loss: 0.1930 - val_accuracy: 0.9279\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 597s 10ms/step - loss: 0.1869 - accuracy: 0.9298 - val_loss: 0.1550 - val_accuracy: 0.9417\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 624s 10ms/step - loss: 0.1352 - accuracy: 0.9514 - val_loss: 0.1015 - val_accuracy: 0.9657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f3824750890>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test['review'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "pred = model.predict(X_test_pad)\n",
    "y_pred = (pred > 0.5)\n",
    "y_pred = y_pred.flatten()*1\n",
    "results = pd.DataFrame({\"id\": id_test, \"sentiment\": y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,id_test = load_all()\n",
    "X_train,X_test= prepare()\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['review'])\n",
    "max_len = max([len(s.split()) for s in X_train['review']])\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_len))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test['review'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "pred = model.predict(X_test_pad)\n",
    "y_pred = (pred > 0.5)\n",
    "y_pred = y_pred.flatten()*1\n",
    "results = pd.DataFrame({\"id\": id_test, \"sentiment\": y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv1D + Glove Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = np.asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    "\n",
    "\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 200))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,id_test = load_all()\n",
    "X_train,X_test= prepare()\n",
    "max_features = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['review'])\n",
    "max_len = max([len(s.split()) for s in X_train['review']])\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "\n",
    "raw_embedding = load_embedding('glove.6B.200d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_layer = Embedding(vocab_size, 200, weights=[embedding_vectors], input_length=max_len, trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/5\n",
      "32000/32000 [==============================] - 185s 6ms/step - loss: 0.5146 - accuracy: 0.7367 - val_loss: 0.4072 - val_accuracy: 0.8214\n",
      "Epoch 2/5\n",
      "32000/32000 [==============================] - 178s 6ms/step - loss: 0.3916 - accuracy: 0.8277 - val_loss: 0.3811 - val_accuracy: 0.8340\n",
      "Epoch 3/5\n",
      "32000/32000 [==============================] - 181s 6ms/step - loss: 0.3376 - accuracy: 0.8553 - val_loss: 0.3829 - val_accuracy: 0.8357\n",
      "Epoch 4/5\n",
      "32000/32000 [==============================] - 179s 6ms/step - loss: 0.2907 - accuracy: 0.8778 - val_loss: 0.4261 - val_accuracy: 0.8259\n",
      "Epoch 5/5\n",
      "32000/32000 [==============================] - 180s 6ms/step - loss: 0.2433 - accuracy: 0.8990 - val_loss: 0.4056 - val_accuracy: 0.8322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd042f5ea50>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test['review'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "pred = model.predict(X_test_pad)\n",
    "y_pred = (pred > 0.5)\n",
    "y_pred = y_pred.flatten()*1\n",
    "results = pd.DataFrame({\"id\": id_test, \"sentiment\": y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding - 0.8347 (do poprawy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 43043: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('./unlabeledTrainData.tsv', delimiter='\\t',error_bad_lines=False)\n",
    "X_train,X_test,y_train,id_test = load_all()\n",
    "df2.drop(['id'],axis=1,inplace=True)\n",
    "df2.columns = ['review']\n",
    "all_rev = pd.concat([X_train,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in all_rev[\"review\"]:\n",
    "    sentences +=review_to_sentences(review, tokenizer) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 469 ms, total: 1min 6s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigrams = Phrases(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 46s, sys: 322 ms, total: 2min 46s\n",
      "Wall time: 2min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigrams = Phrases(sentences=bigrams[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_vector_size = 256\n",
    "trigrams_model = Word2Vec(\n",
    "    sentences = trigrams[bigrams[sentences]],\n",
    "    size = embedding_vector_size,\n",
    "    min_count=3, window=5, workers=4)\n",
    "print(\"Vocabulary size:\", len(trigrams_model.wv.vocab))\n",
    "trigrams_model_name = \"256features\"\n",
    "trigrams_model.save(trigrams_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec.load(\"./256features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,id_test = load_all()\n",
    "X_train,X_test= prepare()\n",
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(X_train['review'])\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_train['review'])\n",
    "max_len = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = model1.wv.vectors,\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim = embedding_matrix[0].shape[0],\n",
    "    output_dim = embedding_matrix[0].shape[1],\n",
    "    input_length = max_len,\n",
    "    weights = [embedding_matrix[0]],\n",
    "    trainable=False))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "#model.summary()\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seq = tokenizer.texts_to_sequences(X_test['review'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "pred = model.predict(X_test_pad)\n",
    "y_pred = (pred > 0.5)\n",
    "y_pred = y_pred.flatten()*1\n",
    "results = pd.DataFrame({\"id\": id_test, \"sentiment\": y_pred})\n",
    "results.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('TF_learn': conda)",
   "language": "python",
   "name": "python37764bittflearncondad80b0f024ab6498da11fd9b9265393ab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
